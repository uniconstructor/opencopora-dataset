{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import mysql.connector\n",
    "# https://github.com/google/sentencepiece\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  passwd=\"root\",\n",
    "  database=\"opencopora\"\n",
    ")\n",
    "sql = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# список id источников\n",
    "source_ids = [\n",
    "    1,\n",
    "    8,\n",
    "    56,\n",
    "    184,\n",
    "    226,\n",
    "    806,\n",
    "    1651,\n",
    "    1675,\n",
    "    1724,\n",
    "    2037,\n",
    "    3469,\n",
    "    3477,\n",
    "    3984,\n",
    "    3994\n",
    "];\n",
    "# получаем все источники документов\n",
    "sql.execute(\"SELECT DISTINCT bk.book_id, bk.book_name, bk.parent_id, bk.syntax_on, src.url, urls.`filename` \\\n",
    "FROM `opencopora.ru`.books bk \\\n",
    "LEFT JOIN `opencopora.ru`.`sources` src \\\n",
    "ON src.book_id=bk.book_id \\\n",
    "LEFT JOIN `opencopora.ru`.`downloaded_urls` urls \\\n",
    "ON src.url=urls.url \\\n",
    "WHERE bk.book_id IN (1,8,56,184,226,806,1651,1675,1724,2037,3469,3477,3984,3994)\")\n",
    " \n",
    "document_sources = pd.DataFrame.from_records(sql.fetchall(), columns=['book_id', 'book_name', 'parent_id', 'syntax_on', 'url', 'filename'])\n",
    "document_sources.set_index('book_id')\n",
    "\n",
    "document_sources.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import wget\n",
    "import shutil\n",
    "import xml.dom.minidom\n",
    "\n",
    "# извлечение всех предложений из документа\n",
    "def extract_sentences(doc_id):\n",
    "    # получаем все предложения из документа, запоминая разбиение на абзацы\n",
    "    sql.execute(\"SELECT sent.sent_id, sent.par_id, sent.`source`, par.book_id, par.pos as par_pos, sent.pos as sent_pos \\\n",
    "    FROM `opencopora.ru`.sentences sent \\\n",
    "    RIGHT JOIN `opencopora.ru`.paragraphs par \\\n",
    "    ON sent.par_id=par.par_id \\\n",
    "    WHERE par.book_id=%s ORDER BY par_pos, sent_pos\", (doc_id,))\n",
    "    return pd.DataFrame.from_records(sql.fetchall(), columns=['sent_id', 'par_id', 'source', 'book_id', 'par_pos', 'sent_pos'])\n",
    "\n",
    "# создание текста из предложений документа\n",
    "def create_text(sentences):\n",
    "    text = ''\n",
    "    for index, row in sentences.iterrows():\n",
    "        text += row['source'] + ' '\n",
    "    return text\n",
    "\n",
    "# создаем автоматическое разбиение\n",
    "def auto_create_sentences(doc_id):\n",
    "    doc_id    = str(doc_id)\n",
    "    directory = get_data_dir(doc_id)\n",
    "    path      = directory + '/' + str(doc_id) + '-text.txt'\n",
    "\n",
    "# получить путь к папке с файлами одного документа\n",
    "def get_data_dir(doc_id):\n",
    "    doc_id    = str(doc_id)\n",
    "    directory = 'dataset/' + doc_id;\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    return directory\n",
    "\n",
    "# получить url сохраненной копии документа\n",
    "def get_saved_source_url(filename):\n",
    "    return 'http://opencorpora.org/files/saved/' + str(filename) + '.html'\n",
    "\n",
    "# сохранение текста, разбитого на предложения вручную\n",
    "def save_sentences(doc_id):\n",
    "    doc_id    = str(doc_id)\n",
    "    directory = get_data_dir(doc_id)\n",
    "    path      = directory + '/' + doc_id + '-sentences.csv'\n",
    "    sentences = extract_sentences(doc_id)\n",
    "    sentences.to_csv(path, index=False, escapechar='\\\\', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    \n",
    "    \n",
    "# сохранение текста, созданного из предложений\n",
    "def save_text(doc_id):\n",
    "    doc_id    = str(doc_id)\n",
    "    directory = get_data_dir(doc_id)\n",
    "    path      = directory + '/' + str(doc_id) + '-text.txt'\n",
    "    text      = create_text(extract_sentences(doc_id))\n",
    "    with open(path, 'w') as txt_file:\n",
    "        txt_file.write(text)\n",
    "\n",
    "# сохранение оригинала документа\n",
    "def save_original_document(filename, doc_id):\n",
    "    doc_id       = str(doc_id)\n",
    "    directory    = get_data_dir(doc_id)\n",
    "    path         = directory + '/' + doc_id + '-original.html'\n",
    "    document_url = get_saved_source_url(filename)\n",
    "    wget.download(document_url, path)\n",
    "    \n",
    "# сохранение данных полной ручной разметки документа в xml\n",
    "def save_annotations(doc_id):\n",
    "    doc_id    = str(doc_id)\n",
    "    src_path  = 'opcorpora-documents/' + doc_id + '.xml'\n",
    "    dest_path = get_data_dir(doc_id) + '/' + doc_id + '-annotations.xml'\n",
    "    # форматируем xml чтобы его было легче читать\n",
    "    raw_xml = xml.dom.minidom.parse(src_path)\n",
    "    with open(dest_path, 'w') as xml_file:\n",
    "        xml_file.write(raw_xml.toprettyxml())\n",
    "        \n",
    "# получение списка документов для каждого источника\n",
    "def get_source_documents(source_id):\n",
    "    # получаем все источники документов\n",
    "    sql.execute(\"SELECT DISTINCT bk.book_id, bk.book_name, bk.parent_id, bk.syntax_on, src.url, urls.`filename` \\\n",
    "    FROM `opencopora.ru`.books bk \\\n",
    "    RIGHT JOIN `opencopora.ru`.`sources` src \\\n",
    "    ON src.book_id=bk.book_id \\\n",
    "    RIGHT JOIN `opencopora.ru`.`downloaded_urls` urls \\\n",
    "    ON src.url=urls.url \\\n",
    "    WHERE bk.parent_id=%s\", (source_id,))\n",
    "    return pd.DataFrame.from_records(sql.fetchall(), columns=['book_id', 'book_name', 'parent_id', 'syntax_on', 'url', 'filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source_id in source_ids:\n",
    "    source_documents = get_source_documents(source_id)\n",
    "    for index, doc in source_documents.iterrows():\n",
    "        if os.path.exists('dataset/' + str(doc['book_id'])):\n",
    "            continue\n",
    "        # сохраняем предложения и текст\n",
    "        try:\n",
    "            save_sentences(doc['book_id'])\n",
    "            save_text(doc['book_id'])\n",
    "        except Exception:\n",
    "            print(\"Не удалось произвести разбор предложений документа\" + str(doc['book_id']))\n",
    "            continue\n",
    "        # сохраняем разметку документа\n",
    "        try:\n",
    "            save_annotations(doc['book_id'])\n",
    "        except Exception:\n",
    "            print(\"Не удалось сохранить аннотацию документа \" + str(doc['book_id']))\n",
    "        # сохраняем оригинал документа\n",
    "        try:\n",
    "            save_original_document(doc['filename'], doc['book_id'])\n",
    "        except Exception:\n",
    "            print(\"Не удалось сохранить оригинал документа \" + str(doc['book_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создать обучающую выборку для автоматического разбиения текста на предложения\n",
    "# создает текстовый файл в который записаны все предложения из всех текстов языкового корпуса\n",
    "def create_train_set():\n",
    "    sql.execute(\"SELECT sent_id, source FROM sentences\")\n",
    "    \n",
    "    train_sentences = pd.DataFrame.from_records(sql.fetchall(), columns=['sent_id', 'source'])\n",
    "    \n",
    "    train_text =''\n",
    "    for index, row in train_sentences.iterrows():\n",
    "        train_text += row['source'] + \"\\n\"\n",
    "        \n",
    "    with open('train.txt', 'w') as txt_file:\n",
    "        txt_file.write(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучение модели\n",
    "# https://github.com/google/sentencepiece/blob/master/python/README.md\n",
    "spm.SentencePieceTrainer.Train('--input=train.txt --model_prefix=m --vocab_size=32000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import mysql.connector\n",
    "import csv\n",
    "import os\n",
    "import wget\n",
    "import xml.dom.minidom\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  passwd=\"root\",\n",
    "  database=\"opencopora.ru\"\n",
    ")\n",
    "sql = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4022\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>book_name</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>syntax_on</th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Частный корреспондент\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://chaskor.ru</td>\n",
       "      <td>4e293b41a67923.57187979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>00021 Школа злословия</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.chaskor.ru/article/shkola_zlosloviy...</td>\n",
       "      <td>4dda53c1719c23.72408057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>00022 Последнее восстание в Сеуле</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.chaskor.ru/article/poslednee_vossta...</td>\n",
       "      <td>4de630ae805107.92951684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>00023 За кота - ответишь!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.chaskor.ru/article/za_kota_-_otveti...</td>\n",
       "      <td>4de630b496c6b1.81320751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>00024 Быстротечный кинороман</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.chaskor.ru/article/bystrotechnyj_ki...</td>\n",
       "      <td>4de630bae523e8.68387316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>00014 Холодная ванна возвращает силы</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.chaskor.ru/article/holodnaya_vanna_...</td>\n",
       "      <td>4dda535864d0f3.68074327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>00031 Рецессия в Латвии и Эстонии</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.chaskor.ru/article/retsessiya_v_lat...</td>\n",
       "      <td>4de630c64a52d1.08149203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Википедия</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://ru.wikipedia.org</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>06037 100 дней Обамы: iТоги</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.chaskor.ru/article/100_dnej_obamy_i...</td>\n",
       "      <td>4de6619227c894.71103195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>01961 100 миллиардов может и не хватить</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.chaskor.ru/article/100_milliardov_m...</td>\n",
       "      <td>4de630d4bc7c33.59812927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_id                                book_name  parent_id  syntax_on  \\\n",
       "0        1                  \"Частный корреспондент\"          0          0   \n",
       "1        2                    00021 Школа злословия          1          0   \n",
       "2        3        00022 Последнее восстание в Сеуле          1          0   \n",
       "3        4                00023 За кота - ответишь!          1          0   \n",
       "4        5             00024 Быстротечный кинороман          1          0   \n",
       "5        6     00014 Холодная ванна возвращает силы          1          0   \n",
       "6        7        00031 Рецессия в Латвии и Эстонии          1          0   \n",
       "7        8                                Википедия          0          0   \n",
       "8        9              06037 100 дней Обамы: iТоги          1          0   \n",
       "9       10  01961 100 миллиардов может и не хватить          1          0   \n",
       "\n",
       "                                                 url                 filename  \n",
       "0                                  http://chaskor.ru  4e293b41a67923.57187979  \n",
       "1  http://www.chaskor.ru/article/shkola_zlosloviy...  4dda53c1719c23.72408057  \n",
       "2  http://www.chaskor.ru/article/poslednee_vossta...  4de630ae805107.92951684  \n",
       "3  http://www.chaskor.ru/article/za_kota_-_otveti...  4de630b496c6b1.81320751  \n",
       "4  http://www.chaskor.ru/article/bystrotechnyj_ki...  4de630bae523e8.68387316  \n",
       "5  http://www.chaskor.ru/article/holodnaya_vanna_...  4dda535864d0f3.68074327  \n",
       "6  http://www.chaskor.ru/article/retsessiya_v_lat...  4de630c64a52d1.08149203  \n",
       "7                            http://ru.wikipedia.org                     None  \n",
       "8  http://www.chaskor.ru/article/100_dnej_obamy_i...  4de6619227c894.71103195  \n",
       "9  http://www.chaskor.ru/article/100_milliardov_m...  4de630d4bc7c33.59812927  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# список отобранных id источников (исключены битые статьи и служебные разделы, а также некорректно размеченные источники)\n",
    "source_ids = [\n",
    "    1,\n",
    "    8,\n",
    "    56,\n",
    "    184,\n",
    "    226,\n",
    "    806,\n",
    "    1651,\n",
    "    1675,\n",
    "    1724,\n",
    "    2037,\n",
    "    3469,\n",
    "    3477,\n",
    "    3984,\n",
    "    3994\n",
    "]\n",
    "# получаем все источники документов\n",
    "sql.execute(\"SELECT DISTINCT bk.book_id, bk.book_name, bk.parent_id, bk.syntax_on, src.url, urls.`filename` \\\n",
    "FROM `opencopora.ru`.books bk \\\n",
    "LEFT JOIN `opencopora.ru`.`sources` src \\\n",
    "ON src.book_id=bk.book_id \\\n",
    "LEFT JOIN `opencopora.ru`.`downloaded_urls` urls \\\n",
    "ON src.url=urls.url\"#\\\n",
    "#WHERE bk.book_id IN (1,8,56,184,226,806,1651,1675,1724,2037,3469,3477,3984,3994)\"\n",
    ")\n",
    " \n",
    "document_sources = pd.DataFrame.from_records(sql.fetchall(), columns=['book_id', 'book_name', 'parent_id', 'syntax_on', 'url', 'filename'])\n",
    "document_sources.set_index('book_id')\n",
    "\n",
    "print(len(document_sources))\n",
    "document_sources.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# извлечение всех предложений из документа\n",
    "def extract_sentences(doc_id):\n",
    "    # получаем из базы все предложения из документа, запоминая разбиение на абзацы\n",
    "    sql.execute(\"SELECT sent.sent_id, sent.par_id, sent.`source`, par.book_id, par.pos as par_pos, sent.pos as sent_pos \\\n",
    "    FROM `opencopora.ru`.sentences sent \\\n",
    "    RIGHT JOIN `opencopora.ru`.paragraphs par \\\n",
    "    ON sent.par_id=par.par_id \\\n",
    "    WHERE par.book_id=%s ORDER BY par_pos, sent_pos\", (doc_id,))\n",
    "    return pd.DataFrame.from_records(sql.fetchall(), columns=['sent_id', 'par_id', 'source', 'book_id', 'par_pos', 'sent_pos'])\n",
    "\n",
    "# создание текста из предложений документа\n",
    "def create_text(sentences):\n",
    "    text = ''\n",
    "    for index, row in sentences.iterrows():\n",
    "        text += row['source'] + ' '\n",
    "    return text\n",
    "\n",
    "# получить путь к папке с файлами одного документа\n",
    "def get_data_dir(doc_id):\n",
    "    doc_id    = str(doc_id)\n",
    "    directory = 'dataset/' + doc_id\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    return directory\n",
    "\n",
    "# получить url сохраненной копии документа\n",
    "def get_saved_source_url(filename):\n",
    "    return 'http://opencorpora.org/files/saved/' + str(filename) + '.html'\n",
    "\n",
    "# сохранение текста, разбитого на предложения вручную\n",
    "def save_sentences(doc_id):\n",
    "    doc_id    = str(doc_id)\n",
    "    directory = get_data_dir(doc_id)\n",
    "    path      = directory + '/' + doc_id + '-sentences.csv'\n",
    "    sentences = extract_sentences(doc_id)\n",
    "    sentences.to_csv(path, index=False, escapechar='\\\\', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    \n",
    "# сохранение текста, созданного из предложений\n",
    "def save_text(doc_id):\n",
    "    doc_id    = str(doc_id)\n",
    "    directory = get_data_dir(doc_id)\n",
    "    path      = directory + '/' + str(doc_id) + '-text.txt'\n",
    "    text      = create_text(extract_sentences(doc_id))\n",
    "    with open(path, 'w') as txt_file:\n",
    "        txt_file.write(text)\n",
    "\n",
    "# сохранение оригинала документа\n",
    "def save_original_document(filename, doc_id):\n",
    "    doc_id       = str(doc_id)\n",
    "    directory    = get_data_dir(doc_id)\n",
    "    path         = directory + '/' + doc_id + '-original.html'\n",
    "    document_url = get_saved_source_url(filename)\n",
    "    wget.download(document_url, path)\n",
    "    \n",
    "# сохранение данных полной ручной разметки документа в xml\n",
    "def save_annotations(doc_id):\n",
    "    doc_id    = str(doc_id)\n",
    "    src_path  = 'opcorpora-documents/' + doc_id + '.xml'\n",
    "    dest_path = get_data_dir(doc_id) + '/' + doc_id + '-annotations.xml'\n",
    "    # форматируем xml чтобы его было легче читать\n",
    "    raw_xml = xml.dom.minidom.parse(src_path)\n",
    "    with open(dest_path, 'w') as xml_file:\n",
    "        xml_file.write(raw_xml.toprettyxml())\n",
    "        \n",
    "# автоматическое разбиение текста на предложения - точка отсчета для сравнения целевых показателей будущей модели\n",
    "# разбивает текст документа на предложения, пользуясь стандартной моделью из NLTK\n",
    "def save_baseline(doc_id):\n",
    "    doc_id = str(doc_id)\n",
    "    # проверяем что папка не служебная\n",
    "    if doc_id.startswith('.'):\n",
    "        return\n",
    "    # папка должна существовать\n",
    "    if not os.path.exists(doc_id):\n",
    "        return FileNotFoundError('ERR: не найдена папка для документа id=' + doc_id)\n",
    "    \n",
    "    text_path = 'dataset/' + doc_id + '/' + doc_id + '-text.txt'\n",
    "    text_file = open(text_path)\n",
    "    text      = text_file.read()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    # производим автоматическое разбиение на предложения для получения baseline\n",
    "    sentences_path = 'dataset/' + doc_id + '/' + doc_id + '-sentences-auto.csv'\n",
    "    df = pd.DataFrame(data={\"source\": sentences})\n",
    "    df.to_csv(sentences_path, sep=',', index=True, quoting=csv.QUOTE_NONNUMERIC, index_label=\"sent_pos\")\n",
    "        \n",
    "# получение списка документов для одного источника\n",
    "def get_source_documents(source_id):\n",
    "    sql.execute(\"SELECT DISTINCT bk.book_id, bk.book_name, bk.parent_id, bk.syntax_on, src.url, urls.`filename` \\\n",
    "    FROM `opencopora.ru`.books bk \\\n",
    "    RIGHT JOIN `opencopora.ru`.`sources` src \\\n",
    "    ON src.book_id=bk.book_id \\\n",
    "    RIGHT JOIN `opencopora.ru`.`downloaded_urls` urls \\\n",
    "    ON src.url=urls.url \\\n",
    "    WHERE bk.parent_id=%s\", (source_id,))\n",
    "    return pd.DataFrame.from_records(sql.fetchall(), columns=['book_id', 'book_name', 'parent_id', 'syntax_on', 'url', 'filename'])\n",
    "\n",
    "# получение данных одного документа по id\n",
    "def get_document_by_id(doc_id):\n",
    "    sql.execute(\"SELECT DISTINCT bk.book_id, bk.book_name, bk.parent_id, bk.syntax_on, src.url, urls.`filename` \\\n",
    "    FROM `opencopora.ru`.books bk \\\n",
    "    RIGHT JOIN `opencopora.ru`.`sources` src \\\n",
    "    ON src.book_id=bk.book_id \\\n",
    "    RIGHT JOIN `opencopora.ru`.`downloaded_urls` urls \\\n",
    "    ON src.url=urls.url \\\n",
    "    WHERE bk.book_id=%s\", (doc_id,))\n",
    "    return pd.DataFrame.from_records(sql.fetchall(), columns=['book_id', 'book_name', 'parent_id', 'syntax_on', 'url', 'filename'])\n",
    "\n",
    "# оценить качество разбиения текста на предложения (от 0 до 100 %)\n",
    "def calculate_split_score(sentence_count, hit_count):\n",
    "    if hit_count == 0:\n",
    "        return 0\n",
    "    return round((hit_count / sentence_count) * 100, 2)\n",
    "    \n",
    "# подсчитать статистику точности разбиения на предложения по одному документу\n",
    "def calculate_document_stats(doc_id):\n",
    "    manual_sentences_file = 'dataset/' + doc_id + '/' + doc_id + '-sentences.csv'\n",
    "    manual_df = pd.DataFrame.from_csv(path=manual_sentences_file)\n",
    "    \n",
    "    auto_sentences_file = 'dataset/' + doc_id + '/' + doc_id + '-sentences-auto.csv'\n",
    "    auto_df = pd.DataFrame.from_csv(path=auto_sentences_file)\n",
    "    \n",
    "    # получаем списки предложений из обучающей и тестовой выборки\n",
    "    manual_sentences = list(manual_df['source'].values)\n",
    "    auto_sentences   = list(auto_df['source'].values)\n",
    "    \n",
    "    # всего совпадений (одинаково разбитых предложений)\n",
    "    total_hits           = set(manual_sentences).intersection(set(auto_sentences))\n",
    "    # определяем сколько (и какие) предложения из ручного разбиения удалось автоматически повторить\n",
    "    manual_split_hits    = set(manual_sentences).intersection(set(auto_sentences))\n",
    "    manual_split_misses  = set(manual_sentences).difference(set(manual_split_hits))\n",
    "    \n",
    "    # определяем сколько (и какие) предложения из автоматического разбиения совпадают с ручным\n",
    "    auto_split_hits    = set(auto_sentences).intersection(set(manual_sentences))\n",
    "    auto_split_misses  = set(manual_sentences).difference(set(manual_split_hits))\n",
    "    \n",
    "    return {\n",
    "        \"doc_id\" : doc_id,\n",
    "        # статистика разбиения\n",
    "        \"stats\" : {\n",
    "            # общее количество предложений которые одинаково разбиты вручную и автоматически\n",
    "            \"total_hit_count\"          : len(total_hits),\n",
    "            # результаты ручного разбиения\n",
    "            \"manual_split_count\"       : len(manual_sentences),\n",
    "            \"manual_split_misses_count\": len(manual_split_misses),\n",
    "            # сколько процентов ручного разбиения удалось повторить автоматически\n",
    "            \"manual_split_score\"       : calculate_split_score(len(manual_sentences), len(manual_split_hits)),\n",
    "            # результаты авторазбиения\n",
    "            \"auto_split_count\"         : len(auto_sentences),\n",
    "            \"auto_split_misses_count\"  : len(auto_split_misses),\n",
    "            # сколько процентов автоматического разбиения соответствует ручной проверке\n",
    "            \"auto_split_score\"         : calculate_split_score(len(auto_sentences), len(auto_split_hits)),\n",
    "        },\n",
    "        # результаты разбиения (текст предложений по группам)\n",
    "        \"results\" : {\n",
    "            \"manual_sentences\"   : manual_sentences,\n",
    "            \"auto_sentences\"     : auto_sentences,\n",
    "            # все предложения, одинаково разбитые и вручную и автоматически\n",
    "            \"total_hits\"         : total_hits,\n",
    "            # количество предложений для которых не удалось построить автоматическое разбиение\n",
    "            \"manual_split_misses\": manual_split_misses,\n",
    "            # количество предложений в автоматическом разбиении которые были выделены неправильно\n",
    "            \"auto_split_misses\"  : auto_split_misses,\n",
    "        } \n",
    "    }\n",
    "\n",
    "# вычислить точность изначального разбиения (без обучения токенизатора)\n",
    "def calculate_baseline_document_score(doc_id):\n",
    "    data = calculate_document_stats(doc_id)\n",
    "    return data[\"stats\"][\"auto_split_score\"]\n",
    "\n",
    "# вычислить точность разбиения обученной модели\n",
    "def calculate_trained_document_score(doc_id, tokenizer):\n",
    "    manual_sentences_file = 'dataset/' + doc_id + '/' + doc_id + '-sentences.csv'\n",
    "    manual_df = pd.DataFrame.from_csv(path=manual_sentences_file)\n",
    "    \n",
    "    trained_sentences_file = 'dataset/' + doc_id + '/' + doc_id + '-sentences-trained.csv'\n",
    "    trained_df = pd.DataFrame.from_csv(path=trained_sentences_file)\n",
    "    \n",
    "    # получаем списки предложений из обучающей и тестовой выборки\n",
    "    manual_sentences = list(manual_df['source'].values)\n",
    "    trained_sentences = list(trained_df['source'].values)\n",
    "    \n",
    "    # всего совпадений (одинаково разбитых предложений)\n",
    "    total_hits           = set(manual_sentences).intersection(set(trained_sentences))\n",
    "    # определяем сколько (и какие) предложения из ручного разбиения удалось автоматически повторить\n",
    "    manual_split_hits    = set(manual_sentences).intersection(set(trained_sentences))\n",
    "    manual_split_misses  = set(manual_sentences).difference(set(manual_split_hits))\n",
    "    \n",
    "    # определяем сколько (и какие) предложения из автоматического разбиения совпадают с ручным\n",
    "    trained_split_hits    = set(trained_sentences).intersection(set(manual_sentences))\n",
    "    trained_split_misses  = set(manual_sentences).difference(set(manual_split_hits))\n",
    "    \n",
    "    trained_result = {\n",
    "        \"doc_id\" : doc_id,\n",
    "        \"stats\" : {\n",
    "            \"total_hit_count\"          : len(total_hits),\n",
    "            \"manual_split_count\"       : len(manual_sentences),\n",
    "            \"manual_split_misses_count\": len(manual_split_misses),\n",
    "            \"manual_split_score\"       : calculate_split_score(len(manual_sentences), len(manual_split_hits)),\n",
    "            \"trained_split_count\"         : len(trained_sentences),\n",
    "            \"trained_split_misses_count\"  : len(trained_split_misses),\n",
    "            \"trained_split_score\"         : calculate_split_score(len(trained_sentences), len(trained_split_hits)),\n",
    "        },\n",
    "        \"results\" : {\n",
    "            \"manual_sentences\"      : manual_sentences,\n",
    "            \"trained_sentences\"     : trained_sentences,\n",
    "            \"total_hits\"            : total_hits,\n",
    "            \"manual_split_misses\"   : manual_split_misses,\n",
    "            \"trained_split_misses\"  : trained_split_misses,\n",
    "        } \n",
    "    }\n",
    "    return trained_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подготовить все данные и признаки для одного документа корпуса\n",
    "def prepare_document(doc_id):\n",
    "    doc_id = str(doc_id)\n",
    "    doc    = get_document_by_id(doc_id)\n",
    "    # получаем из документа все предложения на которые он был разбит вручную\n",
    "    # собираем предложения обратно в текст на котором будем обучать модель для автоматического разбиения\n",
    "    try:\n",
    "        save_sentences(doc_id)\n",
    "        save_text(doc_id)\n",
    "    except Exception:\n",
    "        print(\"Не удалось произвести разбор предложений документа\" + str(doc_id))\n",
    "    # сохраняем разметку документа\n",
    "    try:\n",
    "        save_annotations(doc_id)\n",
    "    except Exception:\n",
    "        print(\"Не удалось сохранить аннотацию документа \" + str(doc_id))\n",
    "    # сохраняем оригинал документа\n",
    "    try:\n",
    "        save_original_document(doc['filename'], doc_id)\n",
    "    except Exception:\n",
    "        print(\"Не удалось сохранить оригинал документа \" + str(doc_id))\n",
    "\n",
    "# создать csv-файл со списком всех документов корпуса, добавить дополнительные признаки для каждого документа\n",
    "def prepare_document_index():\n",
    "    dataset_folders = os.listdir('dataset')\n",
    "    document_scores = []\n",
    "    baseline_scores = []\n",
    "    for folder in tqdm(dataset_folders):\n",
    "        if folder.startswith('.'):\n",
    "            continue\n",
    "        doc_score = calculate_baseline_document_score(folder)\n",
    "        document_scores.append({\n",
    "            \"doc_id\"         : folder,\n",
    "            \"baseline_score\" : doc_score\n",
    "        })\n",
    "        baseline_scores.append(doc_score)\n",
    "        break\n",
    "    df = pd.DataFrame(document_scores)\n",
    "    df.to_csv(\"documents.csv\", sep=',', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    print(\"Mean baseline document score:\")\n",
    "    print(np.mean(baseline_scores))\n",
    "    print(\"Median baseline document score:\")\n",
    "    print(np.median(baseline_scores))\n",
    "    \n",
    "# подготовить обучающую выборку для модели разбора текста на предложения\n",
    "# (Основная функция на этом шаге)\n",
    "def prepare_dataset():\n",
    "    for index, doc in document_sources.iterrows():\n",
    "        # не создаем заново те документы для которых уже извлечены данные\n",
    "        if os.path.exists('dataset/' + str(doc['book_id'])):\n",
    "            continue\n",
    "        # сохраняем предложения и текст\n",
    "        try:\n",
    "            save_sentences(doc['book_id'])\n",
    "            save_text(doc['book_id'])\n",
    "        except Exception:\n",
    "            print(\"Не удалось произвести разбор предложений документа\" + str(doc['book_id']))\n",
    "            continue\n",
    "        # сохраняем разметку документа\n",
    "        try:\n",
    "            save_annotations(doc['book_id'])\n",
    "        except Exception:\n",
    "            print(\"Не удалось сохранить аннотацию документа \" + str(doc['book_id']))\n",
    "        # сохраняем оригинал документа\n",
    "        try:\n",
    "            save_original_document(doc['filename'], doc['book_id'])\n",
    "        except Exception:\n",
    "            print(\"Не удалось сохранить оригинал документа \" + str(doc['book_id']))\n",
    "\n",
    "# @see https://nlpforhackers.io/splitting-text-into-sentences/\n",
    "# обучение nltk-токенизатора для работы с предложениями этого языкового корпуса\n",
    "def train_nltk_tokenizer():\n",
    "    text    = \"\"\n",
    "    # считыввем все предложения корпуа, по 1 на каждой строке\n",
    "    file    = open(\"train.txt\")\n",
    "    lines   = [line.rstrip('\\n') for line in file]\n",
    "    for line in lines:\n",
    "        text += line\n",
    "    # создаем модель для разбиения текста\n",
    "    trainer = PunktTrainer()\n",
    "    trainer.INCLUDE_ALL_COLLOCS = True\n",
    "    trainer.train(text)\n",
    "    # создаем модель, обученную на текстах корпуса\n",
    "    tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "\n",
    "    # Test the tokenizer on a piece of text\n",
    "    #sentences = open('dataset/10/10-text.txt').read()\n",
    "    #print(tokenizer.tokenize(sentences))\n",
    "    # View the learned abbreviations\n",
    "    #print(tokenizer._params.abbrev_types)\n",
    "    # set([...])\n",
    "\n",
    "    # Here's how to debug every split decision\n",
    "    #for decision in tokenizer.debug_decisions(sentences):\n",
    "    #    pprint(decision)\n",
    "    #    print('=' * 30)\n",
    "        \n",
    "    return tokenizer\n",
    "\n",
    "# сохранить разбиение созданное обученной моделью\n",
    "def save_trained(doc_id, tokenizer):\n",
    "    doc_id = str(doc_id)\n",
    "    # проверяем что папка не служебная\n",
    "    if doc_id.startswith('.'):\n",
    "        return\n",
    "    # папка должна существовать\n",
    "    if not os.path.exists('dataset/' + doc_id):\n",
    "        return FileNotFoundError('ERR: не найдена папка для документа id=' + doc_id)\n",
    "    \n",
    "    text_path = 'dataset/' + doc_id + '/' + doc_id + '-text.txt'\n",
    "    text_file = open(text_path)\n",
    "    text      = text_file.read()\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    # производим автоматическое разбиение на предложения для получения baseline\n",
    "    sentences_path = 'dataset/' + doc_id + '/' + doc_id + '-sentences-trained.csv'\n",
    "    df = pd.DataFrame(data={\"source\": sentences})\n",
    "    df.to_csv(sentences_path, sep=',', index=True, quoting=csv.QUOTE_NONNUMERIC, index_label=\"sent_pos\")\n",
    "    \n",
    "    return df\n",
    "   \n",
    "# вычислить и сохранить результат работы модели, обученной на текстах языкового корпуса\n",
    "def save_trained_model_score(tokenizer):\n",
    "    dataset_folders = os.listdir('dataset')\n",
    "    document_scores = []\n",
    "    trained_scores  = []\n",
    "    for folder in tqdm(dataset_folders):\n",
    "        if folder.startswith('.'):\n",
    "            continue\n",
    "        save_trained(folder, tokenizer)\n",
    "        trained_score = calculate_trained_document_score(folder, tokenizer)\n",
    "        document_scores.append({\n",
    "            \"doc_id\"        : folder,\n",
    "            \"trained_score\" : trained_score['stats']['trained_split_score']\n",
    "        })\n",
    "        trained_scores.append(trained_score['stats']['trained_split_score'])\n",
    "        \n",
    "    df = pd.DataFrame(document_scores)\n",
    "    df.to_csv(\"documents-trained.csv\", sep=',', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    print(\"Mean trained document score:\")\n",
    "    print(np.mean(trained_scores))\n",
    "    print(\"Mean trained median score:\")\n",
    "    print(np.median(trained_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1964 [00:00<?, ?it/s]/Users/frost/.conda/envs/opencopora-dataset/lib/python3.5/site-packages/ipykernel_launcher.py:157: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "/Users/frost/.conda/envs/opencopora-dataset/lib/python3.5/site-packages/ipykernel_launcher.py:160: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'dataset/10/10-sentences-trained.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-5074f2521b1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#prepare_document_index()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnew_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_nltk_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msave_trained_model_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-c9a073523c48>\u001b[0m in \u001b[0;36msave_trained_model_score\u001b[0;34m(tokenizer)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0msave_trained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mtrained_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_trained_document_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         trained_scores.append({\n\u001b[1;32m    130\u001b[0m             \u001b[0;34m\"doc_id\"\u001b[0m         \u001b[0;34m:\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-1d4b6da6f1c5>\u001b[0m in \u001b[0;36mcalculate_trained_document_score\u001b[0;34m(doc_id, tokenizer)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mtrained_sentences_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dataset/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdoc_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdoc_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-sentences-trained.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mtrained_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrained_sentences_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;31m# получаем списки предложений из обучающей и тестовой выборки\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/frost/.conda/envs/opencopora-dataset/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfrom_csv\u001b[0;34m(cls, path, header, sep, index_col, parse_dates, encoding, tupleize_cols, infer_datetime_format)\u001b[0m\n\u001b[1;32m   1577\u001b[0m                           \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m                           \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtupleize_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m                           infer_datetime_format=infer_datetime_format)\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'block'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/frost/.conda/envs/opencopora-dataset/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/frost/.conda/envs/opencopora-dataset/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/frost/.conda/envs/opencopora-dataset/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/frost/.conda/envs/opencopora-dataset/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/frost/.conda/envs/opencopora-dataset/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'dataset/10/10-sentences-trained.csv' does not exist"
     ]
    }
   ],
   "source": [
    "new_tokenizer = train_nltk_tokenizer()\n",
    "save_trained_model_score(new_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1964 [00:00<?, ?it/s]/Users/frost/.conda/envs/opencopora-dataset/lib/python3.5/site-packages/ipykernel_launcher.py:157: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "/Users/frost/.conda/envs/opencopora-dataset/lib/python3.5/site-packages/ipykernel_launcher.py:160: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "100%|██████████| 1964/1964 [01:17<00:00, 25.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean trained document score:\n",
      "85.04055017829855\n",
      "Mean trained median score:\n",
      "88.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_trained_model_score(new_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.95318390219053"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создать обучающую выборку для автоматического разбиения текста на предложения\n",
    "# создает текстовый файл в который записаны все предложения из всех текстов языкового корпуса\n",
    "# (каждое предложение с новой строки)\n",
    "def create_sentence_train_set():\n",
    "    sql.execute(\"SELECT sent_id, source FROM sentences\")\n",
    "    # получаем все размеченные вручную предложения из всех текстов корпуса\n",
    "    train_sentences = pd.DataFrame.from_records(sql.fetchall(), columns=['sent_id', 'source'])\n",
    "    train_text      = ''\n",
    "    \n",
    "    for index, row in train_sentences.iterrows():\n",
    "        train_text += row['source'] + \"\\n\"\n",
    "        \n",
    "    with open('sentence-train.txt', 'w') as txt_file:\n",
    "        txt_file.write(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

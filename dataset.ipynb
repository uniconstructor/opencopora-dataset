{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import mysql.connector\n",
    "import csv\n",
    "import os\n",
    "import wget\n",
    "import xml.dom.minidom\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  passwd=\"root\",\n",
    "  database=\"opencopora.ru\"\n",
    ")\n",
    "sql = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>book_name</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>syntax_on</th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Частный корреспондент\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://chaskor.ru</td>\n",
       "      <td>4e293b41a67923.57187979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>00021 Школа злословия</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.chaskor.ru/article/shkola_zlosloviy...</td>\n",
       "      <td>4dda53c1719c23.72408057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>00022 Последнее восстание в Сеуле</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.chaskor.ru/article/poslednee_vossta...</td>\n",
       "      <td>4de630ae805107.92951684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>00023 За кота - ответишь!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.chaskor.ru/article/za_kota_-_otveti...</td>\n",
       "      <td>4de630b496c6b1.81320751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>00024 Быстротечный кинороман</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.chaskor.ru/article/bystrotechnyj_ki...</td>\n",
       "      <td>4de630bae523e8.68387316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>00014 Холодная ванна возвращает силы</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.chaskor.ru/article/holodnaya_vanna_...</td>\n",
       "      <td>4dda535864d0f3.68074327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>00031 Рецессия в Латвии и Эстонии</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.chaskor.ru/article/retsessiya_v_lat...</td>\n",
       "      <td>4de630c64a52d1.08149203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Википедия</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://ru.wikipedia.org</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>06037 100 дней Обамы: iТоги</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.chaskor.ru/article/100_dnej_obamy_i...</td>\n",
       "      <td>4de6619227c894.71103195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>01961 100 миллиардов может и не хватить</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.chaskor.ru/article/100_milliardov_m...</td>\n",
       "      <td>4de630d4bc7c33.59812927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_id                                book_name  parent_id  syntax_on  \\\n",
       "0        1                  \"Частный корреспондент\"          0          0   \n",
       "1        2                    00021 Школа злословия          1          0   \n",
       "2        3        00022 Последнее восстание в Сеуле          1          0   \n",
       "3        4                00023 За кота - ответишь!          1          0   \n",
       "4        5             00024 Быстротечный кинороман          1          0   \n",
       "5        6     00014 Холодная ванна возвращает силы          1          0   \n",
       "6        7        00031 Рецессия в Латвии и Эстонии          1          0   \n",
       "7        8                                Википедия          0          0   \n",
       "8        9              06037 100 дней Обамы: iТоги          1          0   \n",
       "9       10  01961 100 миллиардов может и не хватить          1          0   \n",
       "\n",
       "                                                 url                 filename  \n",
       "0                                  http://chaskor.ru  4e293b41a67923.57187979  \n",
       "1  http://www.chaskor.ru/article/shkola_zlosloviy...  4dda53c1719c23.72408057  \n",
       "2  http://www.chaskor.ru/article/poslednee_vossta...  4de630ae805107.92951684  \n",
       "3  http://www.chaskor.ru/article/za_kota_-_otveti...  4de630b496c6b1.81320751  \n",
       "4  http://www.chaskor.ru/article/bystrotechnyj_ki...  4de630bae523e8.68387316  \n",
       "5  http://www.chaskor.ru/article/holodnaya_vanna_...  4dda535864d0f3.68074327  \n",
       "6  http://www.chaskor.ru/article/retsessiya_v_lat...  4de630c64a52d1.08149203  \n",
       "7                            http://ru.wikipedia.org                     None  \n",
       "8  http://www.chaskor.ru/article/100_dnej_obamy_i...  4de6619227c894.71103195  \n",
       "9  http://www.chaskor.ru/article/100_milliardov_m...  4de630d4bc7c33.59812927  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# список отобранных id источников (исключены битые статьи и служебные разделы, а также некорректно размеченные источники)\n",
    "source_ids = [\n",
    "    1,\n",
    "    8,\n",
    "    56,\n",
    "    184,\n",
    "    226,\n",
    "    806,\n",
    "    1651,\n",
    "    1675,\n",
    "    1724,\n",
    "    2037,\n",
    "    3469,\n",
    "    3477,\n",
    "    3984,\n",
    "    3994\n",
    "]\n",
    "# получаем документы\n",
    "sql.execute(\"SELECT DISTINCT bk.book_id, bk.book_name, bk.parent_id, bk.syntax_on, src.url, urls.`filename` \\\n",
    "FROM `opencopora.ru`.books bk \\\n",
    "LEFT JOIN `opencopora.ru`.`sources` src \\\n",
    "ON src.book_id=bk.book_id \\\n",
    "LEFT JOIN `opencopora.ru`.`downloaded_urls` urls \\\n",
    "ON src.url=urls.url\")\n",
    " \n",
    "document_sources = pd.DataFrame.from_records(sql.fetchall(), columns=['book_id', 'book_name', 'parent_id', 'syntax_on', 'url', 'filename'])\n",
    "document_sources.set_index('book_id')\n",
    "document_sources.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# извлечение всех предложений из документа\n",
    "def extract_sentences(doc_id):\n",
    "    # получаем из базы все предложения из документа, запоминая разбиение на абзацы\n",
    "    sql.execute(\"SELECT sent.sent_id, sent.par_id, sent.`source`, par.book_id, par.pos as par_pos, sent.pos as sent_pos \\\n",
    "    FROM `opencopora.ru`.sentences sent \\\n",
    "    RIGHT JOIN `opencopora.ru`.paragraphs par \\\n",
    "    ON sent.par_id=par.par_id \\\n",
    "    WHERE par.book_id=%s ORDER BY par_pos, sent_pos\", (doc_id,))\n",
    "    return pd.DataFrame.from_records(sql.fetchall(), columns=['sent_id', 'par_id', 'source', 'book_id', 'par_pos', 'sent_pos'])\n",
    "\n",
    "# создание текста из предложений документа\n",
    "def create_text(sentences):\n",
    "    text = ''\n",
    "    for index, row in sentences.iterrows():\n",
    "        text += row['source'] + ' '\n",
    "    return text\n",
    "\n",
    "# получить путь к папке с файлами одного документа\n",
    "def get_data_dir(doc_id):\n",
    "    doc_id    = str(doc_id)\n",
    "    directory = 'dataset/' + doc_id\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    return directory\n",
    "\n",
    "# получить url сохраненной копии документа\n",
    "def get_saved_source_url(filename):\n",
    "    return 'http://opencorpora.org/files/saved/' + str(filename) + '.html'\n",
    "\n",
    "# сохранение текста, разбитого на предложения вручную\n",
    "def save_sentences(doc_id):\n",
    "    doc_id    = str(doc_id)\n",
    "    directory = get_data_dir(doc_id)\n",
    "    path      = directory + '/' + doc_id + '-sentences.csv'\n",
    "    sentences = extract_sentences(doc_id)\n",
    "    sentences.to_csv(path, index=False, escapechar='\\\\', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    \n",
    "# сохранение текста, созданного из предложений\n",
    "def save_text(doc_id):\n",
    "    doc_id    = str(doc_id)\n",
    "    directory = get_data_dir(doc_id)\n",
    "    path      = directory + '/' + str(doc_id) + '-text.txt'\n",
    "    text      = create_text(extract_sentences(doc_id))\n",
    "    with open(path, 'w') as txt_file:\n",
    "        txt_file.write(text)\n",
    "\n",
    "# сохранение оригинала документа\n",
    "def save_original_document(filename, doc_id):\n",
    "    doc_id       = str(doc_id)\n",
    "    directory    = get_data_dir(doc_id)\n",
    "    path         = directory + '/' + doc_id + '-original.html'\n",
    "    document_url = get_saved_source_url(filename)\n",
    "    wget.download(document_url, path)\n",
    "    \n",
    "# сохранение данных полной ручной разметки документа в xml\n",
    "def save_annotations(doc_id):\n",
    "    doc_id    = str(doc_id)\n",
    "    src_path  = 'opcorpora-documents/' + doc_id + '.xml'\n",
    "    dest_path = get_data_dir(doc_id) + '/' + doc_id + '-annotations.xml'\n",
    "    # форматируем xml чтобы его было легче читать\n",
    "    raw_xml = xml.dom.minidom.parse(src_path)\n",
    "    with open(dest_path, 'w') as xml_file:\n",
    "        xml_file.write(raw_xml.toprettyxml())\n",
    "        \n",
    "# автоматическое разбиение текста на предложения - точка отсчета для сравнения целевых показателей будущей модели\n",
    "# разбивает текст документа на предложения, пользуясь стандартной моделью из NLTK\n",
    "def save_baseline(doc_id):\n",
    "    doc_id = str(doc_id)\n",
    "    # проверяем что папка не служебная\n",
    "    if doc_id.startswith('.'):\n",
    "        return\n",
    "    # папка должна существовать\n",
    "    if not os.path.exists(doc_id):\n",
    "        return FileNotFoundError('ERR: не найдена папка для документа id=' + doc_id)\n",
    "    \n",
    "    text_path = 'dataset/' + doc_id + '/' + doc_id + '-text.txt'\n",
    "    text_file = open(text_path)\n",
    "    text      = text_file.read()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    # производим автоматическое разбиение на предложения для получения baseline\n",
    "    sentences_path = 'dataset/' + doc_id + '/' + doc_id + '-sentences-auto.csv'\n",
    "    df = pd.DataFrame(data={\"source\": sentences})\n",
    "    df.to_csv(sentences_path, sep=',', index=True, quoting=csv.QUOTE_NONNUMERIC, index_label=\"sent_pos\")\n",
    "        \n",
    "# получение списка документов для одного источника\n",
    "def get_source_documents(source_id):\n",
    "    sql.execute(\"SELECT DISTINCT bk.book_id, bk.book_name, bk.parent_id, bk.syntax_on, src.url, urls.`filename` \\\n",
    "    FROM `opencopora.ru`.books bk \\\n",
    "    RIGHT JOIN `opencopora.ru`.`sources` src \\\n",
    "    ON src.book_id=bk.book_id \\\n",
    "    RIGHT JOIN `opencopora.ru`.`downloaded_urls` urls \\\n",
    "    ON src.url=urls.url \\\n",
    "    WHERE bk.parent_id=%s\", (source_id,))\n",
    "    return pd.DataFrame.from_records(sql.fetchall(), columns=['book_id', 'book_name', 'parent_id', 'syntax_on', 'url', 'filename'])\n",
    "\n",
    "# получение данных одного документа по id\n",
    "def get_document_by_id(doc_id):\n",
    "    sql.execute(\"SELECT DISTINCT bk.book_id, bk.book_name, bk.parent_id, bk.syntax_on, src.url, urls.`filename` \\\n",
    "    FROM `opencopora.ru`.books bk \\\n",
    "    RIGHT JOIN `opencopora.ru`.`sources` src \\\n",
    "    ON src.book_id=bk.book_id \\\n",
    "    RIGHT JOIN `opencopora.ru`.`downloaded_urls` urls \\\n",
    "    ON src.url=urls.url \\\n",
    "    WHERE bk.book_id=%s\", (doc_id,))\n",
    "    return pd.DataFrame.from_records(sql.fetchall(), columns=['book_id', 'book_name', 'parent_id', 'syntax_on', 'url', 'filename'])\n",
    "\n",
    "# оценить качество разбиения текста на предложения (от 0 до 100 %)\n",
    "def calculate_split_score(sentence_count, hit_count):\n",
    "    if hit_count == 0:\n",
    "        return 0\n",
    "    return round((hit_count / sentence_count) * 100, 2)\n",
    "    \n",
    "# подсчитать статистику точности разбиения на предложения по одному документу\n",
    "def calculate_document_stats(doc_id):\n",
    "    manual_sentences_file = 'dataset/' + doc_id + '/' + doc_id + '-sentences.csv'\n",
    "    manual_df = pd.read_csv(manual_sentences_file)\n",
    "    \n",
    "    auto_sentences_file = 'dataset/' + doc_id + '/' + doc_id + '-sentences-auto.csv'\n",
    "    auto_df = pd.read_csv(auto_sentences_file)\n",
    "    \n",
    "    # получаем списки предложений из обучающей и тестовой выборки\n",
    "    manual_sentences = list(manual_df['source'].values)\n",
    "    auto_sentences   = list(auto_df['source'].values)\n",
    "    \n",
    "    # всего совпадений (одинаково разбитых предложений)\n",
    "    total_hits           = set(manual_sentences).intersection(set(auto_sentences))\n",
    "    # определяем сколько (и какие) предложения из ручного разбиения удалось автоматически повторить\n",
    "    manual_split_hits    = set(manual_sentences).intersection(set(auto_sentences))\n",
    "    manual_split_misses  = set(manual_sentences).difference(set(manual_split_hits))\n",
    "    \n",
    "    # определяем сколько (и какие) предложения из автоматического разбиения совпадают с ручным\n",
    "    auto_split_hits    = set(auto_sentences).intersection(set(manual_sentences))\n",
    "    auto_split_misses  = set(manual_sentences).difference(set(manual_split_hits))\n",
    "    \n",
    "    return {\n",
    "        \"doc_id\" : doc_id,\n",
    "        # статистика разбиения\n",
    "        \"stats\" : {\n",
    "            # общее количество предложений которые одинаково разбиты вручную и автоматически\n",
    "            \"total_hit_count\"          : len(total_hits),\n",
    "            # результаты ручного разбиения\n",
    "            \"manual_split_count\"       : len(manual_sentences),\n",
    "            \"manual_split_misses_count\": len(manual_split_misses),\n",
    "            # сколько процентов ручного разбиения удалось повторить автоматически\n",
    "            \"manual_split_score\"       : calculate_split_score(len(manual_sentences), len(manual_split_hits)),\n",
    "            # результаты авторазбиения\n",
    "            \"auto_split_count\"         : len(auto_sentences),\n",
    "            \"auto_split_misses_count\"  : len(auto_split_misses),\n",
    "            # сколько процентов автоматического разбиения соответствует ручной проверке\n",
    "            \"auto_split_score\"         : calculate_split_score(len(auto_sentences), len(auto_split_hits)),\n",
    "        },\n",
    "        # результаты разбиения (текст предложений по группам)\n",
    "        \"results\" : {\n",
    "            \"manual_sentences\"   : manual_sentences,\n",
    "            \"auto_sentences\"     : auto_sentences,\n",
    "            # все предложения, одинаково разбитые и вручную и автоматически\n",
    "            \"total_hits\"         : total_hits,\n",
    "            # количество предложений для которых не удалось построить автоматическое разбиение\n",
    "            \"manual_split_misses\": manual_split_misses,\n",
    "            # количество предложений в автоматическом разбиении которые были выделены неправильно\n",
    "            \"auto_split_misses\"  : auto_split_misses,\n",
    "        } \n",
    "    }\n",
    "\n",
    "# вычислить точность изначального разбиения (без обучения токенизатора)\n",
    "def calculate_baseline_document_score(doc_id):\n",
    "    data = calculate_document_stats(doc_id)\n",
    "    return data[\"stats\"][\"auto_split_score\"]\n",
    "\n",
    "# вычислить точность разбиения обученной модели\n",
    "def calculate_trained_document_score(doc_id, tokenizer):\n",
    "    manual_sentences_file = 'dataset/' + doc_id + '/' + doc_id + '-sentences.csv'\n",
    "    manual_df = pd.read_csv(manual_sentences_file)\n",
    "    \n",
    "    trained_sentences_file = 'dataset/' + doc_id + '/' + doc_id + '-sentences-trained.csv'\n",
    "    trained_df = pd.read_csv(trained_sentences_file)\n",
    "    \n",
    "    # получаем списки предложений из обучающей и тестовой выборки\n",
    "    manual_sentences = list(manual_df['source'].values)\n",
    "    trained_sentences = list(trained_df['source'].values)\n",
    "    \n",
    "    # всего совпадений (одинаково разбитых предложений)\n",
    "    total_hits           = set(manual_sentences).intersection(set(trained_sentences))\n",
    "    # определяем сколько (и какие) предложения из ручного разбиения удалось автоматически повторить\n",
    "    manual_split_hits    = set(manual_sentences).intersection(set(trained_sentences))\n",
    "    manual_split_misses  = set(manual_sentences).difference(set(manual_split_hits))\n",
    "    \n",
    "    # определяем сколько (и какие) предложения из автоматического разбиения совпадают с ручным\n",
    "    trained_split_hits    = set(trained_sentences).intersection(set(manual_sentences))\n",
    "    trained_split_misses  = set(manual_sentences).difference(set(manual_split_hits))\n",
    "    \n",
    "    trained_result = {\n",
    "        \"doc_id\" : doc_id,\n",
    "        \"stats\" : {\n",
    "            \"total_hit_count\"          : len(total_hits),\n",
    "            \"manual_split_count\"       : len(manual_sentences),\n",
    "            \"manual_split_misses_count\": len(manual_split_misses),\n",
    "            \"manual_split_score\"       : calculate_split_score(len(manual_sentences), len(manual_split_hits)),\n",
    "            \"trained_split_count\"         : len(trained_sentences),\n",
    "            \"trained_split_misses_count\"  : len(trained_split_misses),\n",
    "            \"trained_split_score\"         : calculate_split_score(len(trained_sentences), len(trained_split_hits)),\n",
    "        },\n",
    "        \"results\" : {\n",
    "            \"manual_sentences\"      : manual_sentences,\n",
    "            \"trained_sentences\"     : trained_sentences,\n",
    "            \"total_hits\"            : total_hits,\n",
    "            \"manual_split_misses\"   : manual_split_misses,\n",
    "            \"trained_split_misses\"  : trained_split_misses,\n",
    "        } \n",
    "    }\n",
    "    return trained_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подготовить все данные и признаки для одного документа корпуса\n",
    "def prepare_document(doc_id):\n",
    "    doc_id = str(doc_id)\n",
    "    doc    = get_document_by_id(doc_id)\n",
    "    # получаем из документа все предложения на которые он был разбит вручную\n",
    "    # собираем предложения обратно в текст на котором будем обучать модель для автоматического разбиения\n",
    "    try:\n",
    "        save_sentences(doc_id)\n",
    "        save_text(doc_id)\n",
    "    except Exception:\n",
    "        print(\"Не удалось произвести разбор предложений документа\" + str(doc_id))\n",
    "    # сохраняем разметку документа\n",
    "    try:\n",
    "        save_annotations(doc_id)\n",
    "    except Exception:\n",
    "        print(\"Не удалось сохранить аннотацию документа \" + str(doc_id))\n",
    "    # сохраняем оригинал документа\n",
    "    try:\n",
    "        save_original_document(doc['filename'], doc_id)\n",
    "    except Exception:\n",
    "        print(\"Не удалось сохранить оригинал документа \" + str(doc_id))\n",
    "\n",
    "# создать csv-файл со списком всех документов корпуса, добавить дополнительные признаки для каждого документа\n",
    "def prepare_document_index():\n",
    "    dataset_folders = os.listdir('dataset')\n",
    "    document_scores = []\n",
    "    baseline_scores = []\n",
    "    for folder in tqdm(dataset_folders):\n",
    "        if folder.startswith('.'):\n",
    "            continue\n",
    "        doc_score = calculate_baseline_document_score(folder)\n",
    "        document_scores.append({\n",
    "            \"doc_id\"         : folder,\n",
    "            \"baseline_score\" : doc_score\n",
    "        })\n",
    "        baseline_scores.append(doc_score)\n",
    "    df = pd.DataFrame(document_scores)\n",
    "    df.to_csv(\"documents.csv\", sep=',', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    print(\"Mean baseline document score:\")\n",
    "    print(np.mean(baseline_scores))\n",
    "    print(\"Median baseline document score:\")\n",
    "    print(np.median(baseline_scores))\n",
    "    \n",
    "# подготовить обучающую выборку для модели разбора текста на предложения\n",
    "# (Основная функция на этом шаге)\n",
    "def prepare_dataset():\n",
    "    for index, doc in tqdm(document_sources.iterrows()):\n",
    "        # не создаем заново те документы для которых уже извлечены данные\n",
    "        if os.path.exists('dataset/' + str(doc['book_id'])):\n",
    "            continue\n",
    "        # сохраняем предложения и текст\n",
    "        try:\n",
    "            save_sentences(doc['book_id'])\n",
    "            save_text(doc['book_id'])\n",
    "        except Exception:\n",
    "            print(\"Не удалось произвести разбор предложений документа\" + str(doc['book_id']))\n",
    "            continue\n",
    "        # сохраняем разметку документа\n",
    "        try:\n",
    "            save_annotations(doc['book_id'])\n",
    "        except Exception:\n",
    "            print(\"Не удалось сохранить аннотацию документа \" + str(doc['book_id']))\n",
    "        # сохраняем оригинал документа\n",
    "        try:\n",
    "            save_original_document(doc['filename'], doc['book_id'])\n",
    "        except Exception:\n",
    "            print(\"Не удалось сохранить оригинал документа \" + str(doc['book_id']))\n",
    "\n",
    "# @see https://nlpforhackers.io/splitting-text-into-sentences/\n",
    "# обучение nltk-токенизатора для работы с предложениями этого языкового корпуса\n",
    "def train_nltk_tokenizer():\n",
    "    text    = \"\"\n",
    "    # считыввем все предложения корпуа, по 1 на каждой строке\n",
    "    file    = open(\"train.txt\")\n",
    "    lines   = [line.rstrip('\\n') for line in file]\n",
    "    for line in lines:\n",
    "        text += line\n",
    "    # создаем модель для разбиения текста\n",
    "    trainer = PunktTrainer()\n",
    "    trainer.INCLUDE_ALL_COLLOCS = True\n",
    "    trainer.train(text)\n",
    "    # создаем модель, обученную на текстах корпуса\n",
    "    tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "\n",
    "    # Test the tokenizer on a piece of text\n",
    "    #sentences = open('dataset/10/10-text.txt').read()\n",
    "    #print(tokenizer.tokenize(sentences))\n",
    "    # View the learned abbreviations\n",
    "    #print(tokenizer._params.abbrev_types)\n",
    "    # set([...])\n",
    "\n",
    "    # Here's how to debug every split decision\n",
    "    #for decision in tokenizer.debug_decisions(sentences):\n",
    "    #    pprint(decision)\n",
    "    #    print('=' * 30)\n",
    "        \n",
    "    return tokenizer\n",
    "\n",
    "# сохранить разбиение созданное обученной моделью\n",
    "def save_trained(doc_id, tokenizer):\n",
    "    doc_id = str(doc_id)\n",
    "    # проверяем что папка не служебная\n",
    "    if doc_id.startswith('.'):\n",
    "        return\n",
    "    # папка должна существовать\n",
    "    if not os.path.exists('dataset/' + doc_id):\n",
    "        return FileNotFoundError('ERR: не найдена папка для документа id=' + doc_id)\n",
    "    \n",
    "    text_path = 'dataset/' + doc_id + '/' + doc_id + '-text.txt'\n",
    "    text_file = open(text_path)\n",
    "    text      = text_file.read()\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    # производим автоматическое разбиение на предложения для получения baseline\n",
    "    sentences_path = 'dataset/' + doc_id + '/' + doc_id + '-sentences-trained.csv'\n",
    "    df = pd.DataFrame(data={\"source\": sentences})\n",
    "    df.to_csv(sentences_path, sep=',', index=True, quoting=csv.QUOTE_NONNUMERIC, index_label=\"sent_pos\")\n",
    "    \n",
    "    return df\n",
    "   \n",
    "# вычислить и сохранить результат работы модели, обученной на текстах языкового корпуса\n",
    "def save_trained_model_score(tokenizer):\n",
    "    dataset_folders = os.listdir('dataset')\n",
    "    document_scores = []\n",
    "    trained_scores  = []\n",
    "    for folder in tqdm(dataset_folders):\n",
    "        if folder.startswith('.'):\n",
    "            continue\n",
    "        save_trained(folder, tokenizer)\n",
    "        trained_score = calculate_trained_document_score(folder, tokenizer)\n",
    "        document_scores.append({\n",
    "            \"doc_id\"        : folder,\n",
    "            \"trained_score\" : trained_score['stats']['trained_split_score']\n",
    "        })\n",
    "        trained_scores.append(trained_score['stats']['trained_split_score'])\n",
    "        \n",
    "    df = pd.DataFrame(document_scores)\n",
    "    df.to_csv(\"documents-trained.csv\", sep=',', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    print(\"Mean trained model score:\")\n",
    "    print(np.mean(trained_scores))\n",
    "    print(\"Median trained model score:\")\n",
    "    print(np.median(trained_scores))\n",
    "    \n",
    "# сохранить модель на диск для публикации\n",
    "def save_trained_model_to_file(tokenizer):\n",
    "    f = open('sentence-tokenizer-model.pickle', 'wb')\n",
    "    pickle.dump(tokenizer, f)\n",
    "    f.close()\n",
    "    #tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean baseline document score:\n",
      "83.95318390219053\n",
      "Median baseline document score:\n",
      "87.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108960/108960 [00:00<00:00, 440516.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean trained model score:\n",
      "85.04055017829855\n",
      "Median trained model score:\n",
      "88.24\n"
     ]
    }
   ],
   "source": [
    "prepare_document_index()\n",
    "# обучаем токенизатор на текстах корпуса\n",
    "new_tokenizer = train_nltk_tokenizer()\n",
    "save_trained_model_score(new_tokenizer)\n",
    "save_trained_model_to_file(new_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.95318390219053"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# среднее значение для качества baseline-разбиения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_document_stats(\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создать обучающую выборку для автоматического разбиения текста на предложения\n",
    "# создает текстовый файл в который записаны все предложения из всех текстов языкового корпуса\n",
    "# (каждое предложение с новой строки)\n",
    "def create_sentence_train_set():\n",
    "    sql.execute(\"SELECT sent_id, source FROM sentences\")\n",
    "    # получаем все размеченные вручную предложения из всех текстов корпуса\n",
    "    train_sentences = pd.DataFrame.from_records(sql.fetchall(), columns=['sent_id', 'source'])\n",
    "    train_text      = ''\n",
    "    \n",
    "    for index, row in train_sentences.iterrows():\n",
    "        train_text += row['source'] + \"\\n\"\n",
    "        \n",
    "    with open('sentence-train.txt', 'w') as txt_file:\n",
    "        txt_file.write(train_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
